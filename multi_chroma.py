"""
å¤šæ¨¡å‹åµŒå…¥å‘é‡æ•°æ®åº“å®ç°
æ”¯æŒ OpenAI APIã€DeepSeek APIã€Ollamaæœ¬åœ°æ¨¡å‹ã€sentence-transformersæœ¬åœ°æ¨¡å‹
"""

import os
import re
import ast
import json
import uuid
import httpx
import jieba
from typing import List, Dict, Optional, Any, Union, Callable
from datetime import datetime
from abc import ABC, abstractmethod

import chromadb
from chromadb.config import Settings
from chromadb.api.types import EmbeddingFunction
from dotenv import load_dotenv
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class BaseEmbeddingFunction(EmbeddingFunction, ABC):
    """åµŒå…¥å‡½æ•°åŸºç±»"""
    
    @abstractmethod
    def __call__(self, texts: List[str]) -> List[List[float]]:
        pass


class OpenAIEmbeddingFunction(BaseEmbeddingFunction):
    """OpenAIåµŒå…¥å‡½æ•°"""
    
    def __init__(
        self, 
        api_key: str,
        model: str = "text-embedding-ada-002",
        base_url: str = "https://api.openai.com/v1"
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        
    def __call__(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨OpenAIåµŒå…¥API"""
        try:
            import openai
            
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            response = client.embeddings.create(
                model=self.model,
                input=texts
            )
            
            return [embedding.embedding for embedding in response.data]
            
        except Exception as e:
            print(f"OpenAIåµŒå…¥è°ƒç”¨å¤±è´¥: {e}")
            raise


class DeepSeekEmbeddingFunction(BaseEmbeddingFunction):
    """DeepSeekåµŒå…¥å‡½æ•°"""
    
    def __init__(
        self,
        api_key: str, 
        base_url: str = "https://api.deepseek.com/v1"
    ):
        self.api_key = api_key
        self.base_url = base_url
        
    def __call__(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨DeepSeekåµŒå…¥API"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            embeddings = []
            
            for text in texts:
                payload = {
                    "model": "text-embedding-ada-002",
                    "input": text,
                    "encoding_format": "float"
                }
                
                with httpx.Client() as client:
                    response = client.post(
                        f"{self.base_url}/embeddings",
                        headers=headers,
                        json=payload,
                        timeout=30.0
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        embedding = result["data"][0]["embedding"]
                        embeddings.append(embedding)
                    else:
                        print(f"DeepSeek APIé”™è¯¯: {response.status_code}, {response.text}")
                        # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                        embeddings.append(np.random.rand(1536).tolist())
            
            return embeddings
            
        except Exception as e:
            print(f"DeepSeekåµŒå…¥è°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
            return [np.random.rand(1536).tolist() for _ in texts]


class OllamaEmbeddingFunction(BaseEmbeddingFunction):
    """Ollamaæœ¬åœ°åµŒå…¥å‡½æ•°"""
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "nomic-embed-text"
    ):
        self.base_url = base_url
        self.model = model
        self._test_connection()
        
    def _test_connection(self):
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            with httpx.Client() as client:
                response = client.get(f"{self.base_url}/api/tags", timeout=5.0)
                if response.status_code == 200:
                    print(f"âœ… Ollamaè¿æ¥æˆåŠŸ: {self.base_url}")
                else:
                    raise Exception(f"Ollamaå“åº”é”™è¯¯: {response.status_code}")
        except Exception as e:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: ollama serve")
            print(f"å¹¶å®‰è£…åµŒå…¥æ¨¡å‹: ollama pull {self.model}")
            raise
        
    def __call__(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨Ollamaç”ŸæˆåµŒå…¥"""
        try:
            embeddings = []
            
            for text in texts:
                payload = {
                    "model": self.model,
                    "prompt": text
                }
                
                with httpx.Client() as client:
                    response = client.post(
                        f"{self.base_url}/api/embeddings",
                        json=payload,
                        timeout=30.0
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        embedding = result["embedding"]
                        embeddings.append(embedding)
                    else:
                        print(f"Ollama APIé”™è¯¯: {response.status_code}, {response.text}")
                        # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                        embeddings.append(np.random.rand(768).tolist())
            
            return embeddings
            
        except Exception as e:
            print(f"OllamaåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            return [np.random.rand(768).tolist() for _ in texts]


class LocalEmbeddingFunction(BaseEmbeddingFunction):
    """æœ¬åœ°sentence-transformersåµŒå…¥å‡½æ•°"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        self.model_name = model_name
        self._model = None
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {self.model_name}")
            self._model = SentenceTransformer(self.model_name)
            print(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·å®‰è£…: pip install sentence-transformers")
            raise
    
    def __call__(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”ŸæˆåµŒå…¥"""
        try:
            # é¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬
            processed_texts = [self._preprocess_chinese(text) for text in texts]
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = self._model.encode(
                processed_texts,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            
            return embeddings.tolist()
            
        except Exception as e:
            print(f"æœ¬åœ°åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _preprocess_chinese(self, text: str) -> str:
        """é¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬"""
        try:
            # ä¸­æ–‡åˆ†è¯
            words = jieba.lcut(text)
            return " ".join(words)
        except:
            # å¦‚æœjiebaä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸæ–‡
            return text


class MultiChromaDB:
    """å¤šæ¨¡å‹Chromaå‘é‡æ•°æ®åº“"""
    
    def __init__(
        self,
        embedding_type: str = "local",
        collection_name: str = None,
        persist_directory: str = None,
        **embedding_kwargs
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹å‘é‡æ•°æ®åº“
        
        Args:
            embedding_type: åµŒå…¥ç±»å‹ ("openai", "deepseek", "ollama", "local")
            collection_name: é›†åˆåç§°
            persist_directory: æŒä¹…åŒ–ç›®å½•
            **embedding_kwargs: åµŒå…¥å‡½æ•°çš„é¢å¤–å‚æ•°
        """
        self.embedding_type = embedding_type
        self.collection_name = collection_name or os.getenv("COLLECTION_NAME", "multi_documents")
        self.persist_directory = persist_directory or os.getenv("CHROMA_PERSIST_DIR", "./multi_chroma_db")
        
        # åˆ›å»ºåµŒå…¥å‡½æ•°
        self.embedding_function = self._create_embedding_function(**embedding_kwargs)
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=self.persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        try:
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            print(f"âœ… åŠ è½½ç°æœ‰é›†åˆ: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"embedding_type": self.embedding_type}
            )
            print(f"âœ… åˆ›å»ºæ–°é›†åˆ: {self.collection_name} (åµŒå…¥ç±»å‹: {self.embedding_type})")
    
    def _create_embedding_function(self, **kwargs) -> BaseEmbeddingFunction:
        """åˆ›å»ºåµŒå…¥å‡½æ•°"""
        if self.embedding_type == "openai":
            api_key = kwargs.get("api_key") or os.getenv("OPENAI_API_KEY")
            model = kwargs.get("model") or os.getenv("OPENAI_MODEL", "text-embedding-ada-002")
            base_url = kwargs.get("base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
            
            if not api_key:
                raise ValueError("OpenAI APIå¯†é’¥æœªè®¾ç½®")
            
            return OpenAIEmbeddingFunction(api_key=api_key, model=model, base_url=base_url)
            
        elif self.embedding_type == "deepseek":
            api_key = kwargs.get("api_key") or os.getenv("DEEPSEEK_API_KEY")
            base_url = kwargs.get("base_url") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
            
            if not api_key:
                raise ValueError("DeepSeek APIå¯†é’¥æœªè®¾ç½®")
            
            return DeepSeekEmbeddingFunction(api_key=api_key, base_url=base_url)
            
        elif self.embedding_type == "ollama":
            base_url = kwargs.get("base_url") or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            model = kwargs.get("model") or os.getenv("OLLAMA_MODEL", "nomic-embed-text")
            
            return OllamaEmbeddingFunction(base_url=base_url, model=model)
            
        elif self.embedding_type == "local":
            model_name = kwargs.get("model_name") or os.getenv("LOCAL_MODEL", "sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
            return LocalEmbeddingFunction(model_name=model_name)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥ç±»å‹: {self.embedding_type}")
    
    def process_code_document(self, title: str, content: str, code: str = None) -> str:
        """å¤„ç†åŒ…å«ä»£ç çš„æ–‡æ¡£"""
        enhanced_content = f"æ ‡é¢˜: {title}

æè¿°: {content}"
        
        if code:
            code_features = self._extract_code_features(code)
            
            enhanced_content += f"""

ä»£ç åŠŸèƒ½: {code_features['functionality']}
ç¼–ç¨‹è¯­è¨€: {code_features['language']}
ä¸»è¦å‡½æ•°: {', '.join(code_features['functions'])}
ä¸»è¦ç±»: {', '.join(code_features['classes'])}
å¯¼å…¥æ¨¡å—: {', '.join(code_features['imports'])}
ä»£ç ç±»å‹: {code_features['code_type']}
ä½¿ç”¨åœºæ™¯: {code_features['use_cases']}
æŠ€æœ¯å…³é”®è¯: {' '.join(code_features['keywords'])}

ä»£ç ç¤ºä¾‹:
{code}
"""
        
        return enhanced_content
    
    def _extract_code_features(self, code: str) -> Dict[str, Any]:
        """æå–ä»£ç ç‰¹å¾"""
        features = {
            'functionality': '',
            'language': 'unknown',
            'functions': [],
            'classes': [],
            'imports': [],
            'code_type': 'script',
            'use_cases': '',
            'keywords': []
        }
        
        # æ£€æµ‹ç¼–ç¨‹è¯­è¨€
        if any(keyword in code for keyword in ['def ', 'import ', 'class ', 'python']):
            features['language'] = 'python'
            features = self._analyze_python_code(code, features)
        elif any(keyword in code for keyword in ['function ', 'const ', 'let ', 'var ']):
            features['language'] = 'javascript'
        elif any(keyword in code for keyword in ['public class', 'private ', 'package ']):
            features['language'] = 'java'
        elif 'FROM ' in code.upper() and 'RUN ' in code.upper():
            features['language'] = 'dockerfile'
        
        # æå–é€šç”¨å…³é”®è¯
        code_words = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', code)
        features['keywords'] = list(set(code_words))[:20]
        
        return features
    
    def _analyze_python_code(self, code: str, features: Dict) -> Dict:
        """åˆ†æPythonä»£ç """
        try:
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    features['functions'].append(node.name)
                elif isinstance(node, ast.ClassDef):
                    features['classes'].append(node.name)
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        features['imports'].append(alias.name)
                elif isinstance(node, ast.ImportFrom) and node.module:
                    features['imports'].append(node.module)
            
            # åˆ¤æ–­ä»£ç ç±»å‹å’Œç”¨é€”
            if features['classes']:
                features['code_type'] = 'class'
                features['functionality'] = f"å®šä¹‰äº†{len(features['classes'])}ä¸ªç±»: {', '.join(features['classes'])}"
            elif features['functions']:
                features['code_type'] = 'function'
                features['functionality'] = f"åŒ…å«{len(features['functions'])}ä¸ªå‡½æ•°: {', '.join(features['functions'])}"
            
            # æ¨æ–­ä½¿ç”¨åœºæ™¯
            if any(imp in ['flask', 'django', 'fastapi'] for imp in features['imports']):
                features['use_cases'] = 'Webå¼€å‘, APIå¼€å‘'
            elif any(imp in ['pandas', 'numpy', 'matplotlib'] for imp in features['imports']):
                features['use_cases'] = 'æ•°æ®åˆ†æ, æ•°æ®ç§‘å­¦'
            elif any(imp in ['tensorflow', 'pytorch', 'sklearn'] for imp in features['imports']):
                features['use_cases'] = 'æœºå™¨å­¦ä¹ , äººå·¥æ™ºèƒ½'
            
        except Exception as e:
            print(f"ä»£ç åˆ†æå‡ºé”™: {e}")
        
        return features

    
    def add_documents(
        self,
        documents: List[str],
        metadatas: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None,
        codes: Optional[List[str]] = None
    ) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        if not documents:
            raise ValueError("æ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in documents]
        
        if metadatas is None:
            metadatas = [{} for _ in documents]
        
        # å¤„ç†æ–‡æ¡£å†…å®¹
        processed_documents = []
        for i, doc in enumerate(documents):
            if codes and i < len(codes) and codes[i]:
                title = metadatas[i].get('title', f'æ–‡æ¡£ {i+1}')
                processed_doc = self.process_code_document(title, doc, codes[i])
            else:
                processed_doc = doc
            
            processed_documents.append(processed_doc)
        
        # æ·»åŠ æ—¶é—´æˆ³å’Œç±»å‹
        for metadata in metadatas:
            metadata['added_at'] = datetime.now().isoformat()
            metadata['embedding_type'] = self.embedding_type
        
        try:
            self.collection.add(
                documents=processed_documents,
                metadatas=metadatas,
                ids=ids
            )
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ (åµŒå…¥ç±»å‹: {self.embedding_type})")
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def search(
        self,
        query: str,
        n_results: int = 10,
        where: Optional[Dict] = None,
        include_metadata: bool = True
    ) -> Dict[str, List]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        try:
            include_list = ["documents", "distances", "metadatas"] if include_metadata else ["documents", "distances"]
            
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where,
                include=include_list
            )
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return {"documents": [[]], "distances": [[]], "metadatas": [[]]}
    
    def smart_search(self, query: str, max_results: int = 5) -> List[Dict]:
        """æ™ºèƒ½æœç´¢"""
        query_lower = query.lower()
        results_list = []
        
        # åŸºç¡€è¯­ä¹‰æœç´¢
        base_results = self.search(query, n_results=max_results * 2)
        
        # æ¨æ–­è¿‡æ»¤æ¡ä»¶
        filters = self._infer_filters(query_lower)
        
        # è¿‡æ»¤æœç´¢
        if filters:
            for filter_condition in filters:
                filtered_results = self.search(
                    query, 
                    n_results=max_results,
                    where=filter_condition
                )
                results_list.extend(self._format_results(filtered_results))
        
        # åˆå¹¶ç»“æœ
        results_list.extend(self._format_results(base_results))
        
        # å»é‡
        unique_results = []
        seen_docs = set()
        
        for result in results_list:
            doc_key = result['document'][:100]
            if doc_key not in seen_docs:
                seen_docs.add(doc_key)
                unique_results.append(result)
                
                if len(unique_results) >= max_results:
                    break
        
        return unique_results
    
    def _infer_filters(self, query_lower: str) -> List[Dict]:
        """æ¨æ–­è¿‡æ»¤æ¡ä»¶"""
        filters = []
        
        # ç¼–ç¨‹è¯­è¨€
        language_keywords = {
            'python': ['python', 'py', 'django', 'flask', 'pandas'],
            'javascript': ['javascript', 'js', 'node', 'react', 'vue'],
            'java': ['java', 'spring'],
            'sql': ['sql', 'mysql', 'postgresql']
        }
        
        for lang, keywords in language_keywords.items():
            if any(kw in query_lower for kw in keywords):
                filters.append({"programming_language": lang})
        
        # æŠ€æœ¯é¢†åŸŸ
        domain_keywords = {
            'webå¼€å‘': ['web', 'ç½‘ç«™', 'api', 'å‰ç«¯', 'åç«¯'],
            'æ•°æ®ç§‘å­¦': ['æ•°æ®', 'data', 'åˆ†æ', 'æœºå™¨å­¦ä¹ '],
            'äººå·¥æ™ºèƒ½': ['ai', 'äººå·¥æ™ºèƒ½', 'ml', 'æ·±åº¦å­¦ä¹ ']
        }
        
        for domain, keywords in domain_keywords.items():
            if any(kw in query_lower for kw in keywords):
                filters.append({"domain": domain})
        
        return filters
    
    def _format_results(self, results: Dict) -> List[Dict]:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        formatted = []
        
        if not results['documents'] or not results['documents'][0]:
            return formatted
        
        documents = results['documents'][0]
        distances = results['distances'][0]
        metadatas = results.get('metadatas', [{}])[0] if results.get('metadatas') else [{}] * len(documents)
        
        for doc, distance, metadata in zip(documents, distances, metadatas):
            # æå–æ ‡é¢˜
            title = metadata.get('title', 'Unknown')
            if title == 'Unknown' and 'æ ‡é¢˜:' in doc:
                title_match = re.search(r'æ ‡é¢˜:\s*(.+)', doc)
                if title_match:
                    title = title_match.group(1).strip()
            
            formatted.append({
                'title': title,
                'document': doc,
                'distance': distance,
                'similarity': 1 - distance,
                'metadata': metadata
            })
        
        return formatted
    
    def get_collection_info(self) -> Dict:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                'name': self.collection_name,
                'document_count': count,
                'embedding_type': self.embedding_type,
                'persist_directory': self.persist_directory
            }
        except Exception as e:
            print(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def delete_documents(self, ids: List[str]) -> None:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            self.collection.delete(ids=ids)
            print(f"âœ… æˆåŠŸåˆ é™¤ {len(ids)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
    
    def reset_collection(self) -> None:
        """é‡ç½®é›†åˆï¼ˆåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰"""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"embedding_type": self.embedding_type}
            )
            print(f"âœ… é‡ç½®é›†åˆ: {self.collection_name}")
        except Exception as e:
            print(f"âŒ é‡ç½®é›†åˆå¤±è´¥: {e}")


def create_multi_client(
    embedding_type: str = None,
    collection_name: str = None,
    persist_directory: str = None,
    **kwargs
) -> MultiChromaDB:
    """åˆ›å»ºå¤šæ¨¡å‹Chromaå®¢æˆ·ç«¯"""
    
    # ä»ç¯å¢ƒå˜é‡è·å–é»˜è®¤é…ç½®
    embedding_type = embedding_type or os.getenv("EMBEDDING_TYPE", "local")
    
    return MultiChromaDB(
        embedding_type=embedding_type,
        collection_name=collection_name,
        persist_directory=persist_directory,
        **kwargs
    )


if __name__ == "__main__":
    # å¿«é€Ÿæµ‹è¯•
    try:
        print("ğŸ”„ æ­£åœ¨æµ‹è¯•å¤šæ¨¡å‹å‘é‡æ•°æ®åº“...")
        
        # æ£€æµ‹å¯ç”¨çš„åµŒå…¥ç±»å‹
        embedding_type = os.getenv("EMBEDDING_TYPE", "local")
        print(f"ä½¿ç”¨åµŒå…¥ç±»å‹: {embedding_type}")
        
        db = create_multi_client(embedding_type=embedding_type)
        info = db.get_collection_info()
        print(f"âœ… æ•°æ®åº“ä¿¡æ¯: {info}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")
